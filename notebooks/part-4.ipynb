{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac866250",
   "metadata": {},
   "source": [
    "# Part 4 — Machine Learning\n",
    "\n",
    "## Links\n",
    "- **Live updated app:** https://ind320-project-work-nonewthing.streamlit.app/\n",
    "- **Repo:** https://github.com/TaoM29/IND320-dashboard-basics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd831b05",
   "metadata": {},
   "source": [
    "## AI Usage\n",
    "\n",
    "I Used an ChatGPT 5 as a coding and troubleshooting partner. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1e8b3c",
   "metadata": {},
   "source": [
    "## Work Log\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "118d89c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports, paths, env\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Make PySpark use THIS Python\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "os.environ.setdefault(\"PYSPARK_HADOOP_VERSION\", \"without\")\n",
    "\n",
    "# Cassandra contact info (Docker on Mac)\n",
    "CASS_HOST = \"127.0.0.1\"\n",
    "CASS_PORT = \"9042\"\n",
    "CASS_DC   = \"datacenter1\"\n",
    "KS        = \"ind320\"\n",
    "\n",
    "# Mongo (optional – set our secrets)\n",
    "MONGO_URI = os.getenv(\"MONGO_URI\", \"\")\n",
    "MONGO_DB  = os.getenv(\"MONGO_DB\", \"ind320\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e15b7b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cassandra: Test Cluster | datacenter1 | 5.0.6\n",
      "Tables in ind320: ['consumption_mba_hour', 'production_mba_hour']\n"
     ]
    }
   ],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "\n",
    "cluster = Cluster([\"127.0.0.1\"], port=9042)\n",
    "session = cluster.connect()\n",
    "row = session.execute(\"SELECT cluster_name,data_center,release_version FROM system.local\").one()\n",
    "print(\"✅ Cassandra:\", row.cluster_name, \"|\", row.data_center, \"|\", row.release_version)\n",
    "\n",
    "session.set_keyspace(\"ind320\")\n",
    "tables = [r.table_name for r in session.execute(\n",
    "    \"SELECT table_name FROM system_schema.tables WHERE keyspace_name='ind320'\"\n",
    ")]\n",
    "print(\"Tables in ind320:\", sorted(tables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf72e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/19 13:23:45 WARN Utils: Your hostname, Taofiks-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 10.42.69.134 instead (on interface en0)\n",
      "25/11/19 13:23:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/taom/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/taom/.ivy2/jars\n",
      "com.datastax.spark#spark-cassandra-connector-assembly_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-39f684d9-a17b-4a19-889f-c23ba2a1b733;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-assembly_2.12;3.5.1 in central\n",
      ":: resolution report :: resolve 54ms :: artifacts dl 2ms\n",
      "\t:: modules in use:\n",
      "\tcom.datastax.spark#spark-cassandra-connector-assembly_2.12;3.5.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-39f684d9-a17b-4a19-889f-c23ba2a1b733\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/miniconda3/envs/IND320env/lib/python3.13/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0 artifacts copied, 1 already retrieved (0kB/3ms)\n",
      "25/11/19 13:23:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark ready (assembly).\n"
     ]
    }
   ],
   "source": [
    "# Spark session (DataStax connector assembly contains the Java driver)\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"IND320-Cassandra\")\n",
    "    .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector-assembly_2.12:3.5.1\")\n",
    "    .config(\"spark.cassandra.connection.host\", CASS_HOST)\n",
    "    .config(\"spark.cassandra.connection.port\", CASS_PORT)\n",
    "    .config(\"spark.cassandra.connection.localDC\", CASS_DC)\n",
    "    .config(\"spark.sql.extensions\", \"com.datastax.spark.connector.CassandraSparkExtensions\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(\"Spark ready (assembly).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69c78011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from datetime import datetime, timezone\n",
    "import os, requests, pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Constants\n",
    "BASE_V0 = \"https://api.elhub.no/energy-data/v0\"\n",
    "ELHUB_API_TOKEN = os.getenv(\"ELHUB_API_TOKEN\")  \n",
    "PRICE_AREAS = [\"NO1\",\"NO2\",\"NO3\",\"NO4\",\"NO5\"]\n",
    "\n",
    "\n",
    "# Common headers for JSON:API\n",
    "def headers_jsonapi():\n",
    "    h = {\"Accept\": \"application/vnd.api+json\"}\n",
    "    if ELHUB_API_TOKEN:\n",
    "        h[\"Authorization\"] = f\"Bearer {ELHUB_API_TOKEN}\"\n",
    "    return h\n",
    "\n",
    "# ISO 8601 UTC offset formatting\n",
    "def iso_utc_offset(dt: datetime) -> str:\n",
    "    if dt.tzinfo is None:\n",
    "        dt = dt.replace(tzinfo=timezone.utc)\n",
    "    dt = dt.astimezone(timezone.utc)\n",
    "    off = dt.strftime(\"%z\")\n",
    "    off = off[:-2] + \":\" + off[-2:]\n",
    "    return dt.strftime(\"%Y-%m-%dT%H:%M:%S\") + off\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f33f7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Production groups: ['solar', 'hydro', 'wind', 'thermal', 'nuclear', 'other']\n",
      "Consumption groups: ['household', 'cabin', 'primary', 'secondary', 'tertiary', 'industry', 'private', 'business']\n"
     ]
    }
   ],
   "source": [
    "# Groups (ids)\n",
    "def list_groups(kind=\"production\"):\n",
    "    url = f\"{BASE_V0}/{kind}-groups\"\n",
    "    r = requests.get(url, headers=headers_jsonapi(), timeout=30)\n",
    "    r.raise_for_status()\n",
    "    rows = []\n",
    "    for item in r.json().get(\"data\", []):\n",
    "        attrs = item.get(\"attributes\", {}) or {}\n",
    "        rows.append({\"id\": item.get(\"id\"), \"name\": attrs.get(\"name\")})\n",
    "    df = pd.DataFrame(rows)\n",
    "  \n",
    "    ids = [g for g in df[\"id\"].tolist() if g != \"*\"]\n",
    "    return ids, df\n",
    "\n",
    "prod_group_ids, production_groups_df = list_groups(\"production\")\n",
    "cons_group_ids, consumption_groups_df = list_groups(\"consumption\")\n",
    "\n",
    "print(\"Production groups:\", prod_group_ids)\n",
    "print(\"Consumption groups:\", cons_group_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3775691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic monthly fetch\n",
    "def fetch_month_generic(\n",
    "    price_area: str,\n",
    "    group_id: str,\n",
    "    year: int,\n",
    "    month: int,\n",
    "    dataset: str,\n",
    "    group_param_name: str,\n",
    "    inner_key: str,\n",
    "    group_col_out: str,\n",
    "    verbose: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    start = datetime(year, month, 1, tzinfo=timezone.utc)\n",
    "    end   = datetime(year + (month==12), (month % 12) + 1, 1, tzinfo=timezone.utc)\n",
    "\n",
    "    params = {\n",
    "        \"dataset\": dataset,\n",
    "        \"priceArea\": price_area,\n",
    "        group_param_name: group_id,\n",
    "        \"startDate\": iso_utc_offset(start),\n",
    "        \"endDate\":   iso_utc_offset(end),\n",
    "        \"pageSize\":  10000,\n",
    "    }\n",
    "\n",
    "    url = f\"{BASE_V0}/price-areas\"\n",
    "    r = requests.get(url, headers=headers_jsonapi(), params=params, timeout=90)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"HTTP\", r.status_code, \"|\", r.headers.get(\"Content-Type\"))\n",
    "        print(\"URL:\", r.url)\n",
    "\n",
    "    if r.status_code != 200:\n",
    "        if verbose: print(\"Body preview:\", r.text[:400])\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    data = r.json().get(\"data\", [])\n",
    "    if not data:\n",
    "        return pd.DataFrame(columns=[\"priceArea\", group_col_out, \"startTime\", \"quantityKwh\"])\n",
    "\n",
    "    rows = []\n",
    "    for rec in data:\n",
    "        attrs = rec.get(\"attributes\", {}) or {}\n",
    "        area  = attrs.get(\"name\") or rec.get(\"id\") or price_area\n",
    "        inner = attrs.get(inner_key, []) or []\n",
    "        for item in inner:\n",
    "            rows.append({\n",
    "                \"priceArea\": area,\n",
    "                group_col_out: item.get(group_col_out),\n",
    "                \"startTime\": item.get(\"startTime\"),\n",
    "                \"quantityKwh\": item.get(\"quantityKwh\")\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    df[\"startTime\"]   = pd.to_datetime(df[\"startTime\"], utc=True, errors=\"coerce\")\n",
    "    df[\"quantityKwh\"] = pd.to_numeric(df[\"quantityKwh\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"startTime\",\"quantityKwh\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Thin wrappers (so our code stays readable)\n",
    "def fetch_month_prod(area, group_id, year, month, verbose=False):\n",
    "    return fetch_month_generic(\n",
    "        area, group_id, year, month,\n",
    "        dataset=\"PRODUCTION_PER_GROUP_MBA_HOUR\",\n",
    "        group_param_name=\"productionGroup\",\n",
    "        inner_key=\"productionPerGroupMbaHour\",\n",
    "        group_col_out=\"productionGroup\",\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "def fetch_month_cons(area, group_id, year, month, verbose=False):\n",
    "    return fetch_month_generic(\n",
    "        area, group_id, year, month,\n",
    "        dataset=\"CONSUMPTION_PER_GROUP_MBA_HOUR\",\n",
    "        group_param_name=\"consumptionGroup\",\n",
    "        inner_key=\"consumptionPerGroupMbaHour\",\n",
    "        group_col_out=\"consumptionGroup\",\n",
    "        verbose=verbose,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76132b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planned requests (prod): 1080 months = 5 areas × 6 groups × 3 years × 12 months\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa60e161517444ba963b705e9829408b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Production (2022–2024) months:   0%|          | 0/1080 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PRODUCTION 2022–2024 ===\n",
      "Requests run: 1080 | Non-empty months: 900 | Rows: 657,600\n",
      "Span: 2021-12-31 23:00:00+00:00 → 2024-12-31 22:00:00+00:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>priceArea</th>\n",
       "      <th>productionGroup</th>\n",
       "      <th>startTime</th>\n",
       "      <th>quantityKwh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NO1</td>\n",
       "      <td>solar</td>\n",
       "      <td>2021-12-31 23:00:00+00:00</td>\n",
       "      <td>6.448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NO1</td>\n",
       "      <td>solar</td>\n",
       "      <td>2022-01-01 00:00:00+00:00</td>\n",
       "      <td>6.062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NO1</td>\n",
       "      <td>solar</td>\n",
       "      <td>2022-01-01 01:00:00+00:00</td>\n",
       "      <td>4.697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NO1</td>\n",
       "      <td>solar</td>\n",
       "      <td>2022-01-01 02:00:00+00:00</td>\n",
       "      <td>10.907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NO1</td>\n",
       "      <td>solar</td>\n",
       "      <td>2022-01-01 03:00:00+00:00</td>\n",
       "      <td>5.975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  priceArea productionGroup                 startTime  quantityKwh\n",
       "0       NO1           solar 2021-12-31 23:00:00+00:00        6.448\n",
       "1       NO1           solar 2022-01-01 00:00:00+00:00        6.062\n",
       "2       NO1           solar 2022-01-01 01:00:00+00:00        4.697\n",
       "3       NO1           solar 2022-01-01 02:00:00+00:00       10.907\n",
       "4       NO1           solar 2022-01-01 03:00:00+00:00        5.975"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fetch production data for 2022–2024\n",
    "YEARS_PROD = [2022, 2023, 2024]\n",
    "\n",
    "total_months = len(PRICE_AREAS) * len(prod_group_ids) * len(YEARS_PROD) * 12\n",
    "print(f\"Planned requests (prod): {total_months} months \"\n",
    "      f\"= {len(PRICE_AREAS)} areas × {len(prod_group_ids)} groups × {len(YEARS_PROD)} years × 12 months\")\n",
    "\n",
    "parts, runs, non_empty, row_count = [], 0, 0, 0\n",
    "pbar = tqdm(total=total_months, desc=\"Production (2022–2024) months\", leave=True)\n",
    "\n",
    "for area in PRICE_AREAS:\n",
    "    for g in prod_group_ids:\n",
    "        for y, m in itertools.product(YEARS_PROD, range(1, 13)):\n",
    "            df_m = fetch_month_prod(area, g, y, m)\n",
    "            runs += 1\n",
    "            if not df_m.empty:\n",
    "                parts.append(df_m)\n",
    "                non_empty += 1\n",
    "                row_count += len(df_m)\n",
    "            # update progress every month\n",
    "            pbar.set_postfix_str(f\"rows_so_far={row_count:,}\")\n",
    "            pbar.update(1)\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "prod_2224 = (pd.concat(parts, ignore_index=True)\n",
    "             if parts else pd.DataFrame(columns=[\"priceArea\",\"productionGroup\",\"startTime\",\"quantityKwh\"]))\n",
    "prod_2224 = prod_2224.drop_duplicates(subset=[\"priceArea\",\"productionGroup\",\"startTime\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n=== PRODUCTION 2022–2024 ===\")\n",
    "print(f\"Requests run: {runs} | Non-empty months: {non_empty} | Rows: {len(prod_2224):,}\")\n",
    "print(\"Span:\", prod_2224[\"startTime\"].min() if not prod_2224.empty else None,\n",
    "      \"→\",  prod_2224[\"startTime\"].max() if not prod_2224.empty else None)\n",
    "display(prod_2224.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e83b4980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Planned requests (cons): 1920 months = 5 areas × 8 groups × 4 years × 12 months\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5d1db6e494e4c93ae775afd0715bfa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Consumption (2021–2024) months:   0%|          | 0/1920 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CONSUMPTION 2021–2024 ===\n",
      "Requests run: 1920 | Non-empty months: 1200 | Rows: 876,600\n",
      "Span: 2020-12-31 23:00:00+00:00 → 2024-12-31 22:00:00+00:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>priceArea</th>\n",
       "      <th>consumptionGroup</th>\n",
       "      <th>startTime</th>\n",
       "      <th>quantityKwh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NO1</td>\n",
       "      <td>household</td>\n",
       "      <td>2020-12-31 23:00:00+00:00</td>\n",
       "      <td>2366888.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NO1</td>\n",
       "      <td>household</td>\n",
       "      <td>2021-01-01 00:00:00+00:00</td>\n",
       "      <td>2325218.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NO1</td>\n",
       "      <td>household</td>\n",
       "      <td>2021-01-01 01:00:00+00:00</td>\n",
       "      <td>2273791.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NO1</td>\n",
       "      <td>household</td>\n",
       "      <td>2021-01-01 02:00:00+00:00</td>\n",
       "      <td>2221311.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NO1</td>\n",
       "      <td>household</td>\n",
       "      <td>2021-01-01 03:00:00+00:00</td>\n",
       "      <td>2188174.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  priceArea consumptionGroup                 startTime  quantityKwh\n",
       "0       NO1        household 2020-12-31 23:00:00+00:00    2366888.8\n",
       "1       NO1        household 2021-01-01 00:00:00+00:00    2325218.2\n",
       "2       NO1        household 2021-01-01 01:00:00+00:00    2273791.2\n",
       "3       NO1        household 2021-01-01 02:00:00+00:00    2221311.8\n",
       "4       NO1        household 2021-01-01 03:00:00+00:00    2188174.2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fetch consumption data for 2021–2024\n",
    "YEARS_CONS = [2021, 2022, 2023, 2024]\n",
    "\n",
    "total_months_c = len(PRICE_AREAS) * len(cons_group_ids) * len(YEARS_CONS) * 12\n",
    "print(f\"\\nPlanned requests (cons): {total_months_c} months \"\n",
    "      f\"= {len(PRICE_AREAS)} areas × {len(cons_group_ids)} groups × {len(YEARS_CONS)} years × 12 months\")\n",
    "\n",
    "parts_c, runs_c, non_empty_c, row_count_c = [], 0, 0, 0\n",
    "pbar_c = tqdm(total=total_months_c, desc=\"Consumption (2021–2024) months\", leave=True)\n",
    "\n",
    "for area in PRICE_AREAS:\n",
    "    for g in cons_group_ids:\n",
    "        for y, m in itertools.product(YEARS_CONS, range(1, 13)):\n",
    "            df_m = fetch_month_cons(area, g, y, m)\n",
    "            runs_c += 1\n",
    "            if not df_m.empty:\n",
    "                parts_c.append(df_m)\n",
    "                non_empty_c += 1\n",
    "                row_count_c += len(df_m)\n",
    "            pbar_c.set_postfix_str(f\"rows_so_far={row_count_c:,}\")\n",
    "            pbar_c.update(1)\n",
    "\n",
    "pbar_c.close()\n",
    "\n",
    "cons_2124 = (pd.concat(parts_c, ignore_index=True)\n",
    "             if parts_c else pd.DataFrame(columns=[\"priceArea\",\"consumptionGroup\",\"startTime\",\"quantityKwh\"]))\n",
    "cons_2124 = cons_2124.drop_duplicates(subset=[\"priceArea\",\"consumptionGroup\",\"startTime\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n=== CONSUMPTION 2021–2024 ===\")\n",
    "print(f\"Requests run: {runs_c} | Non-empty months: {non_empty_c} | Rows: {len(cons_2124):,}\")\n",
    "print(\"Span:\", cons_2124[\"startTime\"].min() if not cons_2124.empty else None,\n",
    "      \"→\",  cons_2124[\"startTime\"].max() if not cons_2124.empty else None)\n",
    "display(cons_2124.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78488557",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o37.applySchemaToPythonRDD.\n: java.lang.NoSuchMethodError: 'scala.collection.immutable.Seq org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.expressions()'\n\tat org.apache.spark.sql.cassandra.CassandraMetaDataRule$.findMetadataExpressions(CassandraMetadataFunctions.scala:191)\n\tat org.apache.spark.sql.cassandra.CassandraMetaDataRule$$anonfun$apply$1.applyOrElse(CassandraMetadataFunctions.scala:196)\n\tat org.apache.spark.sql.cassandra.CassandraMetaDataRule$$anonfun$apply$1.applyOrElse(CassandraMetadataFunctions.scala:195)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:405)\n\tat org.apache.spark.sql.cassandra.CassandraMetaDataRule$.apply(CassandraMetadataFunctions.scala:195)\n\tat org.apache.spark.sql.cassandra.CassandraMetaDataRule$.apply(CassandraMetadataFunctions.scala:102)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:226)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:222)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:173)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:222)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:188)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:209)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)\n\tat org.apache.spark.sql.SparkSession.internalCreateDataFrame(SparkSession.scala:572)\n\tat org.apache.spark.sql.SparkSession.applySchemaToPythonRDD(SparkSession.scala:877)\n\tat org.apache.spark.sql.SparkSession.applySchemaToPythonRDD(SparkSession.scala:862)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m      5\u001b[39m prod_pdf = (\n\u001b[32m      6\u001b[39m     prod_2224\n\u001b[32m      7\u001b[39m     .rename(columns={\u001b[33m\"\u001b[39m\u001b[33mpriceArea\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33mprice_area\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mproductionGroup\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33mproduction_group\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m     )[[\u001b[33m\"\u001b[39m\u001b[33mprice_area\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mproduction_group\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33myear\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mstart_time\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mquantity_kwh\u001b[39m\u001b[33m\"\u001b[39m]]\n\u001b[32m     15\u001b[39m )\n\u001b[32m     17\u001b[39m prod_schema = T.StructType([\n\u001b[32m     18\u001b[39m     T.StructField(\u001b[33m\"\u001b[39m\u001b[33mprice_area\u001b[39m\u001b[33m\"\u001b[39m,       T.StringType(),  \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m     19\u001b[39m     T.StructField(\u001b[33m\"\u001b[39m\u001b[33mproduction_group\u001b[39m\u001b[33m\"\u001b[39m, T.StringType(),  \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m     T.StructField(\u001b[33m\"\u001b[39m\u001b[33mquantity_kwh\u001b[39m\u001b[33m\"\u001b[39m,     T.DoubleType(),  \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m     23\u001b[39m ])\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m prod_sdf = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprod_pdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprod_schema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m prod_sdf.printSchema()\n\u001b[32m     27\u001b[39m prod_sdf.show(\u001b[32m3\u001b[39m, truncate=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/IND320env/lib/python3.13/site-packages/pyspark/sql/session.py:1440\u001b[39m, in \u001b[36mSparkSession.createDataFrame\u001b[39m\u001b[34m(self, data, schema, samplingRatio, verifySchema)\u001b[39m\n\u001b[32m   1436\u001b[39m     data = pd.DataFrame(data, columns=column_names)\n\u001b[32m   1438\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd.DataFrame):\n\u001b[32m   1439\u001b[39m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1440\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSparkSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[32m   1441\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\n\u001b[32m   1442\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1443\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_dataframe(\n\u001b[32m   1444\u001b[39m     data, schema, samplingRatio, verifySchema  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m   1445\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/IND320env/lib/python3.13/site-packages/pyspark/sql/pandas/conversion.py:363\u001b[39m, in \u001b[36mSparkConversionMixin.createDataFrame\u001b[39m\u001b[34m(self, data, schema, samplingRatio, verifySchema)\u001b[39m\n\u001b[32m    361\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    362\u001b[39m converted_data = \u001b[38;5;28mself\u001b[39m._convert_from_pandas(data, schema, timezone)\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconverted_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/IND320env/lib/python3.13/site-packages/pyspark/sql/session.py:1488\u001b[39m, in \u001b[36mSparkSession._create_dataframe\u001b[39m\u001b[34m(self, data, schema, samplingRatio, verifySchema)\u001b[39m\n\u001b[32m   1486\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1487\u001b[39m jrdd = \u001b[38;5;28mself\u001b[39m._jvm.SerDeUtil.toJavaArray(rdd._to_java_object_rdd())\n\u001b[32m-> \u001b[39m\u001b[32m1488\u001b[39m jdf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jsparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapplySchemaToPythonRDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjrdd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstruct\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1489\u001b[39m df = DataFrame(jdf, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1490\u001b[39m df._schema = struct\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/IND320env/lib/python3.13/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/IND320env/lib/python3.13/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/IND320env/lib/python3.13/site-packages/py4j/protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o37.applySchemaToPythonRDD.\n: java.lang.NoSuchMethodError: 'scala.collection.immutable.Seq org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.expressions()'\n\tat org.apache.spark.sql.cassandra.CassandraMetaDataRule$.findMetadataExpressions(CassandraMetadataFunctions.scala:191)\n\tat org.apache.spark.sql.cassandra.CassandraMetaDataRule$$anonfun$apply$1.applyOrElse(CassandraMetadataFunctions.scala:196)\n\tat org.apache.spark.sql.cassandra.CassandraMetaDataRule$$anonfun$apply$1.applyOrElse(CassandraMetadataFunctions.scala:195)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:405)\n\tat org.apache.spark.sql.cassandra.CassandraMetaDataRule$.apply(CassandraMetadataFunctions.scala:195)\n\tat org.apache.spark.sql.cassandra.CassandraMetaDataRule$.apply(CassandraMetadataFunctions.scala:102)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:226)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:222)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:173)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:222)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:188)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:209)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)\n\tat org.apache.spark.sql.SparkSession.internalCreateDataFrame(SparkSession.scala:572)\n\tat org.apache.spark.sql.SparkSession.applySchemaToPythonRDD(SparkSession.scala:877)\n\tat org.apache.spark.sql.SparkSession.applySchemaToPythonRDD(SparkSession.scala:862)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "# === PRODUCTION ===\n",
    "prod_pdf = (\n",
    "    prod_2224\n",
    "    .rename(columns={\"priceArea\":\"price_area\", \"productionGroup\":\"production_group\",\n",
    "                     \"startTime\":\"start_time\", \"quantityKwh\":\"quantity_kwh\"})\n",
    "    .assign(\n",
    "        year=lambda d: d[\"start_time\"].dt.year,\n",
    "        # make timestamps naive UTC for Spark→Cassandra\n",
    "        start_time=lambda d: pd.to_datetime(d[\"start_time\"], utc=True).dt.tz_convert(\"UTC\").dt.tz_localize(None),\n",
    "        quantity_kwh=lambda d: pd.to_numeric(d[\"quantity_kwh\"], errors=\"coerce\")\n",
    "    )[[\"price_area\",\"production_group\",\"year\",\"start_time\",\"quantity_kwh\"]]\n",
    ")\n",
    "\n",
    "prod_schema = T.StructType([\n",
    "    T.StructField(\"price_area\",       T.StringType(),  False),\n",
    "    T.StructField(\"production_group\", T.StringType(),  False),\n",
    "    T.StructField(\"year\",             T.IntegerType(), False),\n",
    "    T.StructField(\"start_time\",       T.TimestampType(), False),\n",
    "    T.StructField(\"quantity_kwh\",     T.DoubleType(),  False),\n",
    "])\n",
    "\n",
    "prod_sdf = spark.createDataFrame(prod_pdf.dropna(), schema=prod_schema)\n",
    "prod_sdf.printSchema()\n",
    "prod_sdf.show(3, truncate=False)\n",
    "\n",
    "# === CONSUMPTION ===\n",
    "cons_pdf = (\n",
    "    cons_2124\n",
    "    .rename(columns={\"priceArea\":\"price_area\", \"consumptionGroup\":\"consumption_group\",\n",
    "                     \"startTime\":\"start_time\", \"quantityKwh\":\"quantity_kwh\"})\n",
    "    .assign(\n",
    "        year=lambda d: d[\"start_time\"].dt.year,\n",
    "        start_time=lambda d: pd.to_datetime(d[\"start_time\"], utc=True).dt.tz_convert(\"UTC\").dt.tz_localize(None),\n",
    "        quantity_kwh=lambda d: pd.to_numeric(d[\"quantity_kwh\"], errors=\"coerce\")\n",
    "    )[[\"price_area\",\"consumption_group\",\"year\",\"start_time\",\"quantity_kwh\"]]\n",
    ")\n",
    "\n",
    "cons_schema = T.StructType([\n",
    "    T.StructField(\"price_area\",         T.StringType(),  False),\n",
    "    T.StructField(\"consumption_group\",  T.StringType(),  False),\n",
    "    T.StructField(\"year\",               T.IntegerType(), False),\n",
    "    T.StructField(\"start_time\",         T.TimestampType(), False),\n",
    "    T.StructField(\"quantity_kwh\",       T.DoubleType(),  False),\n",
    "])\n",
    "\n",
    "cons_sdf = spark.createDataFrame(cons_pdf.dropna(), schema=cons_schema)\n",
    "cons_sdf.printSchema()\n",
    "cons_sdf.show(3, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524f816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRODUCTION (append after our 2021 data)\n",
    "(prod_sdf.write\n",
    " .format(\"org.apache.spark.sql.cassandra\")\n",
    " .options(keyspace=\"ind320\", table=\"production_mba_hour\")\n",
    " .mode(\"append\")\n",
    " .save()\n",
    ")\n",
    "\n",
    "# CONSUMPTION (new table created: full 2021–2024)\n",
    "(cons_sdf.write\n",
    " .format(\"org.apache.spark.sql.cassandra\")\n",
    " .options(keyspace=\"ind320\", table=\"consumption_mba_hour\")\n",
    " .mode(\"append\")\n",
    " .save()\n",
    ")\n",
    "print(\"Cassandra writes done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IND320env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
