{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac866250",
   "metadata": {},
   "source": [
    "# Part 4 â€” Machine Learning\n",
    "\n",
    "## Links\n",
    "- **Live updated app:** https://ind320-dashboard-basics-newthing.streamlit.app\n",
    "- **Repo:** https://github.com/TaoM29/IND320-dashboard-basics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd831b05",
   "metadata": {},
   "source": [
    "## AI Usage\n",
    "\n",
    "I Used an ChatGPT 5 as a coding and troubleshooting partner: \n",
    "\n",
    "\tâ€¢\tHelped me with implementation of 20_Price_Areas_Map_Selector.py with Folium + GeoJSON, click-to-store coordinates, and choropleth coloring by mean energy values over user-selected intervals.\n",
    "\n",
    "\tâ€¢\tModified Snow_Drift.py page using ERA5 (Open-Meteo) at the clicked coordinate, defining season as Julâ†’Jun, plotting seasonal Qt and a 16-sector wind rose, plus monthly-vs-yearly bonus. \n",
    "\n",
    "\tâ€¢\tHelp with Building Sliding Correlation page (this task): selectable meteorological + energy series, rolling window, lag control, Plotly output, and notebook-friendly guidance. \n",
    "\n",
    "\tâ€¢\tHelp with builing SARIMAX Forecast page with full parameter control and dynamic forecasting, added exogenous weather variables and strategies for future exog (bonus).\n",
    "\n",
    "\tâ€¢\tRefactored multiple static plots to Plotly; added helper links between related pages.\n",
    "\n",
    "\tâ€¢\ttooltips, and side-bar improvements for clarity and consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0752076f",
   "metadata": {},
   "source": [
    "## Work Log\n",
    "\n",
    "### Introduction.\n",
    "This part focused on expanding the dataset window (Elhub 2021â€“2024 + ERA5), reshaping the Streamlit app for clearer navigation, and adding analysis pages that connect meteorology with energy. The app now has four logical sections (Exploration, Regional & Local, Modelling, Quality & Diagnostics), more descriptive page names, and cross-links for a smoother workflow. Heavy operations are cached and show progress indicators. Below is a concise record of what was implemented and refined.\n",
    "\n",
    "\n",
    "â¸»\n",
    "\n",
    "### Spark vs. Cassandra Driver\n",
    "\tâ€¢\tFollowed the assignment scope:\n",
    "\n",
    "\tâ€¢\tProduction: PRODUCTION_PER_GROUP_MBA_HOUR for 2022â€“2024 (all price areas).\n",
    "\tâ€¢\tConsumption: CONSUMPTION_PER_GROUP_MBA_HOUR for 2021â€“2024 (all price areas).\n",
    "\tâ€¢\tAPI fetching mirrors Part 2 (monthly paging), expanded across years and areas.\n",
    "\tâ€¢\tSpark + Cassandra: despite working Spark sessions (connector 3.5.1 variants, shaded/non-shaded JARs, Java 11), the JVM stack intermittently failed to create CqlSession / see the system keyspace reliably in this environment.\n",
    "\tâ€¢\tPer course guidance, switched writes to the official Python Cassandra driver (idempotent bulk upserts).\n",
    "MongoDB writes use bulk upserts + compound unique indexes so the notebook is re-runnable without duplicates.\n",
    "\n",
    "\n",
    "â¸»\n",
    "\n",
    "### App structure, naming & navigation\n",
    "\tâ€¢\tRenamed pages to clearer, more descriptive titles.\n",
    "\t\n",
    "\tâ€¢\tIntroduced four sidebar sections that match the appâ€™s structure:\n",
    "\tâ€¢\tðŸ”Ž Exploration\n",
    "\tâ€¢\tðŸ—ºï¸ Regional & Local\n",
    "\tâ€¢\tðŸ“ˆ Modelling\n",
    "\tâ€¢\tðŸ§ª Quality & Diagnostics\n",
    "\t\n",
    "\tâ€¢\tHome and About mirror these sections with concise descriptions and links.\t\n",
    "\tâ€¢\tCross-page links where it helps the flow (Price Areas Map â†’ Snow Drift after a map click).\n",
    "\tâ€¢\tKept the global Price Area / Year context (set on Price Area Selector) and tiny â€œChange area/yearâ€ links on analysis pages.\n",
    "\n",
    "\n",
    "â¸»\n",
    "\n",
    "### Exploration\n",
    "\tâ€¢\tWeather Overview â€” Stats & Sparklines\n",
    "\tâ€¢\tWhole-year summary table (min/mean/max) + first-month sparkline per variable.\n",
    "\tâ€¢\tAdded: monthly climatology by year for each weather variable.\n",
    "\tâ€¢\tAdded: for wind direction, reused the wind-rose component from Snow Drift so directionality is visualized.\n",
    "\tâ€¢\tRemoved the old generic â€œinteractive chartâ€ (now coveredâ€”betterâ€”by the Weather Explorer page).\n",
    "\tâ€¢\tData from Open-Meteo (ERA5), cached.\n",
    "\tâ€¢\tWeather Explorer â€” Multi-Series & Resampling\n",
    "\tâ€¢\tMulti-select variables, interactive Plotly, and legend/hover.\n",
    "\tâ€¢\tImproved: resample (H/D/W), smooth (rolling mean), and a month range filter for quick seasonal zoom-ins.\n",
    "\tâ€¢\tData from Open-Meteo (ERA5), cached.\n",
    "\tâ€¢\tEnergy Production / Energy Consumption\n",
    "\tâ€¢\tOverview line charts and basic summaries using MongoDB aggregates, respects global Area/Year.\n",
    "\n",
    "\n",
    "â¸»\n",
    "\n",
    "### Regional & Local\n",
    "\tâ€¢\tPrice Areas Map â€” Choropleth & Click-to-Select\n",
    "\tâ€¢\tLoads exported Elspot areas (GeoJSON), auto-detects the property holding NO1â€“NO5, draws outlines, highlights the selected area, and colors a choropleth by mean kWh over user-selected interval/group (production or consumption).\n",
    "\tâ€¢\tClick-to-store coordinate in session state; shows a quick link to Snow Drift for that point.\n",
    "\tâ€¢\tSnow Drift (Tabler)\n",
    "\tâ€¢\tUses ERA5 at the clicked coordinate. Season defined as 1 July â†’ 30 June.\n",
    "\tâ€¢\tComputes Qt per season, shows a 16-sector wind rose, and includes the bonus: monthly Qt vs. yearly Qt overlay.\n",
    "\n",
    "\tâ€¢\tAdded an optional fence-height calculator (Wyoming / Slat-and-wire / Solid). To make the snowâ€drift results actionable, given the estimated annual transport Q_t, the calculator converts that into the required fence height for common designs (Wyoming, Slat-and-wire, Solid) using their storage capacity factors. It shows how different fence types change the needed height, linking the analysis to a practical mitigation choice.\n",
    "\n",
    "\n",
    "â¸»\n",
    "\n",
    "### Modelling\n",
    "\tâ€¢\tSliding Correlation (Weather â†” Energy)\n",
    "\tâ€¢\tSelect meteorological and energy series, lag (lead/lag interpretation) and window length, frequency, and date span.\n",
    "\tâ€¢\tPlotly rolling correlation line with hover, used to explore correlation shifts during normal periods vs. extreme events (see below).\n",
    "\tâ€¢\tSARIMAX Forecast (Energy)\n",
    "\tâ€¢\tFull control of (p,d,q)(P,D,Q,s), training window, forecast horizon, and dynamic in-sample forecasting (with a percentage dynamic start).\n",
    "\tâ€¢\tBonus completed: exogenous weather variables selectable; future exog generated via â€œlastâ€ or â€œhour-of-day/day-of-week meanâ€ strategies.\n",
    "\tâ€¢\tPlots actuals, fitted (in-sample), forecast mean, and 95% CI.\n",
    "\n",
    "\n",
    "####  Scenario 1 - Consumption â†’ household vs temperature_2m (Â°C) (NO1, Jan-2024)\n",
    "\n",
    "Setup\n",
    "\tâ€¢\tConsumption â†’ household vs temperature_2m (Â°C)\n",
    "\tâ€¢\tWindow length 168 h (1 week), lag 0 h.\n",
    "\n",
    "What I saw:\n",
    "\n",
    "\tâ€¢\tThe rolling correlation stays negative most of the month (â‰ˆ âˆ’0.6 to âˆ’0.3), dipping to around âˆ’0.8 mid/late January during the cold spell.\n",
    "\tâ€¢\tOn the overlay, consumption spikes when temperature drops, and eases as temperatures riseâ€”clear winter heating behavior.\n",
    "\tâ€¢\tNudging the lag a few hours (Â±6 h) didnâ€™t flip the sign and only slightly changed magnitude, suggesting near-immediate response of household demand to temperature (little delay).\n",
    "\n",
    "Interpretation:\n",
    "In normal winter conditions, household demand is strongly inversely related to temperatureâ€”colder â†’ more kWh. As the month warms toward the end, the correlation weakens toward zero, likely as temperature variance shrinks and weekday/behavioral patterns contribute more to demand.\n",
    "\n",
    "\n",
    "#### Scenario 2 â€” Wind production vs. wind speed (NO1, Oct-2024)\n",
    "\n",
    "Setup\n",
    "\n",
    "\tâ€¢\tEnergy kind: Production â†’ group wind\n",
    "\tâ€¢\tWeather variable: wind_speed_10m (m/s)\n",
    "\tâ€¢\tWindow: 168 h (1 week)\n",
    "\tâ€¢\tLag: +1 h (weather leads production by ~1 hour)\n",
    "\n",
    "What I saw\n",
    "\n",
    "\tâ€¢\tThe rolling correlation starts high and positive (~0.6â€“0.7) in early October, consistent with wind speed driving wind output.\n",
    "\tâ€¢\tAround Oct 18â€“21, correlation drops toward 0 and briefly negative, then rebounds to ~0.5â€“0.6.\n",
    "\tâ€¢\tThe +1 h lag slightly improved the positive segments versus 0 h, suggesting a short response delay from wind to generation.\n",
    "\n",
    "Interpretation:\n",
    "Strong positive r is expected (more wind â†’ more production). The mid-month dip likely reflects anomalous periods: calm spells or very high winds causing turbine cut-outs where speed no longer maps linearly to output Weekly window balances noise and responsiveness, shorter windows (72 h) made the drop sharper, while longer (336 h) smoothed it but hid the event.\n",
    "\n",
    "\n",
    "â¸»\n",
    "\n",
    "### Quality & Diagnostics\n",
    "\tâ€¢\tTime-Series Analysis â€” STL Decomposition & Spectrogram\n",
    "\tâ€¢\tSTL seasonal/trend decomposition and STFT spectrogram (with clearer spacing/labels).\n",
    "\tâ€¢\tSupports both production and consumption after refactor.\n",
    "\tâ€¢\tData Quality â€” SPC (Outliers) & LOF (Anomalies)\n",
    "\tâ€¢\tSPC-style control bands and Local Outlier Factor highlights: explanatory captions and consistent titles.\n",
    "\n",
    "\n",
    "â¸»\n",
    "\n",
    "### Caching & UX\n",
    "\tâ€¢\tHeavy data paths use @st.cache_data (ERA5 loaders, energy spans, some Mongo aggregates).\n",
    "\tâ€¢\tSpinners/progress on costly steps (data fetch, model fit) so waiting time is visible.\n",
    "\tâ€¢\tThe result: pages stay responsive.\n",
    "\n",
    "\n",
    "â¸»\n",
    "\n",
    "### Bonus completion recap\n",
    "\tâ€¢\tMonthly snow drift overlaid with yearly Qt on Snow Drift.\n",
    "\tâ€¢\tExogenous weather variables integrated into SARIMAX with simple future-exog strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "118d89c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports, paths, env\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import os, requests, pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import itertools\n",
    "from pyspark.sql import functions as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c78011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "BASE_V0 = \"https://api.elhub.no/energy-data/v0\"\n",
    "ELHUB_API_TOKEN = os.getenv(\"ELHUB_API_TOKEN\")  \n",
    "PRICE_AREAS = [\"NO1\",\"NO2\",\"NO3\",\"NO4\",\"NO5\"]\n",
    "\n",
    "\n",
    "# Common headers for JSON:API\n",
    "def headers_jsonapi():\n",
    "    h = {\"Accept\": \"application/vnd.api+json\"}\n",
    "    if ELHUB_API_TOKEN:\n",
    "        h[\"Authorization\"] = f\"Bearer {ELHUB_API_TOKEN}\"\n",
    "    return h\n",
    "\n",
    "# ISO 8601 UTC offset formatting\n",
    "def iso_utc_offset(dt: datetime) -> str:\n",
    "    if dt.tzinfo is None:\n",
    "        dt = dt.replace(tzinfo=timezone.utc)\n",
    "    dt = dt.astimezone(timezone.utc)\n",
    "    off = dt.strftime(\"%z\")\n",
    "    off = off[:-2] + \":\" + off[-2:]\n",
    "    return dt.strftime(\"%Y-%m-%dT%H:%M:%S\") + off\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f33f7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Production groups: ['solar', 'hydro', 'wind', 'thermal', 'nuclear', 'other']\n",
      "Consumption groups: ['household', 'cabin', 'primary', 'secondary', 'tertiary', 'industry', 'private', 'business']\n"
     ]
    }
   ],
   "source": [
    "# Groups (ids)\n",
    "def list_groups(kind=\"production\"):\n",
    "    url = f\"{BASE_V0}/{kind}-groups\"\n",
    "    r = requests.get(url, headers=headers_jsonapi(), timeout=30)\n",
    "    r.raise_for_status()\n",
    "    rows = []\n",
    "    for item in r.json().get(\"data\", []):\n",
    "        attrs = item.get(\"attributes\", {}) or {}\n",
    "        rows.append({\"id\": item.get(\"id\"), \"name\": attrs.get(\"name\")})\n",
    "    df = pd.DataFrame(rows)\n",
    "  \n",
    "    ids = [g for g in df[\"id\"].tolist() if g != \"*\"]\n",
    "    return ids, df\n",
    "\n",
    "prod_group_ids, production_groups_df = list_groups(\"production\")\n",
    "cons_group_ids, consumption_groups_df = list_groups(\"consumption\")\n",
    "\n",
    "print(\"Production groups:\", prod_group_ids)\n",
    "print(\"Consumption groups:\", cons_group_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3775691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic monthly fetch\n",
    "def fetch_month_generic(\n",
    "    price_area: str,\n",
    "    group_id: str,\n",
    "    year: int,\n",
    "    month: int,\n",
    "    dataset: str,\n",
    "    group_param_name: str,\n",
    "    inner_key: str,\n",
    "    group_col_out: str,\n",
    "    verbose: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    start = datetime(year, month, 1, tzinfo=timezone.utc)\n",
    "    end   = datetime(year + (month==12), (month % 12) + 1, 1, tzinfo=timezone.utc)\n",
    "\n",
    "    params = {\n",
    "        \"dataset\": dataset,\n",
    "        \"priceArea\": price_area,\n",
    "        group_param_name: group_id,\n",
    "        \"startDate\": iso_utc_offset(start),\n",
    "        \"endDate\":   iso_utc_offset(end),\n",
    "        \"pageSize\":  10000,\n",
    "    }\n",
    "\n",
    "    url = f\"{BASE_V0}/price-areas\"\n",
    "    r = requests.get(url, headers=headers_jsonapi(), params=params, timeout=90)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"HTTP\", r.status_code, \"|\", r.headers.get(\"Content-Type\"))\n",
    "        print(\"URL:\", r.url)\n",
    "\n",
    "    if r.status_code != 200:\n",
    "        if verbose: print(\"Body preview:\", r.text[:400])\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    data = r.json().get(\"data\", [])\n",
    "    if not data:\n",
    "        return pd.DataFrame(columns=[\"priceArea\", group_col_out, \"startTime\", \"quantityKwh\"])\n",
    "\n",
    "    rows = []\n",
    "    for rec in data:\n",
    "        attrs = rec.get(\"attributes\", {}) or {}\n",
    "        area  = attrs.get(\"name\") or rec.get(\"id\") or price_area\n",
    "        inner = attrs.get(inner_key, []) or []\n",
    "        for item in inner:\n",
    "            rows.append({\n",
    "                \"priceArea\": area,\n",
    "                group_col_out: item.get(group_col_out),\n",
    "                \"startTime\": item.get(\"startTime\"),\n",
    "                \"quantityKwh\": item.get(\"quantityKwh\")\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    df[\"startTime\"]   = pd.to_datetime(df[\"startTime\"], utc=True, errors=\"coerce\")\n",
    "    df[\"quantityKwh\"] = pd.to_numeric(df[\"quantityKwh\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"startTime\",\"quantityKwh\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Thin wrappers (so our code stays readable)\n",
    "def fetch_month_prod(area, group_id, year, month, verbose=False):\n",
    "    return fetch_month_generic(\n",
    "        area, group_id, year, month,\n",
    "        dataset=\"PRODUCTION_PER_GROUP_MBA_HOUR\",\n",
    "        group_param_name=\"productionGroup\",\n",
    "        inner_key=\"productionPerGroupMbaHour\",\n",
    "        group_col_out=\"productionGroup\",\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "def fetch_month_cons(area, group_id, year, month, verbose=False):\n",
    "    return fetch_month_generic(\n",
    "        area, group_id, year, month,\n",
    "        dataset=\"CONSUMPTION_PER_GROUP_MBA_HOUR\",\n",
    "        group_param_name=\"consumptionGroup\",\n",
    "        inner_key=\"consumptionPerGroupMbaHour\",\n",
    "        group_col_out=\"consumptionGroup\",\n",
    "        verbose=verbose,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76132b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planned requests (prod): 1080 months = 5 areas Ã— 6 groups Ã— 3 years Ã— 12 months\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa60e161517444ba963b705e9829408b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Production (2022â€“2024) months:   0%|          | 0/1080 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PRODUCTION 2022â€“2024 ===\n",
      "Requests run: 1080 | Non-empty months: 900 | Rows: 657,600\n",
      "Span: 2021-12-31 23:00:00+00:00 â†’ 2024-12-31 22:00:00+00:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>priceArea</th>\n",
       "      <th>productionGroup</th>\n",
       "      <th>startTime</th>\n",
       "      <th>quantityKwh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NO1</td>\n",
       "      <td>solar</td>\n",
       "      <td>2021-12-31 23:00:00+00:00</td>\n",
       "      <td>6.448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NO1</td>\n",
       "      <td>solar</td>\n",
       "      <td>2022-01-01 00:00:00+00:00</td>\n",
       "      <td>6.062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NO1</td>\n",
       "      <td>solar</td>\n",
       "      <td>2022-01-01 01:00:00+00:00</td>\n",
       "      <td>4.697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NO1</td>\n",
       "      <td>solar</td>\n",
       "      <td>2022-01-01 02:00:00+00:00</td>\n",
       "      <td>10.907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NO1</td>\n",
       "      <td>solar</td>\n",
       "      <td>2022-01-01 03:00:00+00:00</td>\n",
       "      <td>5.975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  priceArea productionGroup                 startTime  quantityKwh\n",
       "0       NO1           solar 2021-12-31 23:00:00+00:00        6.448\n",
       "1       NO1           solar 2022-01-01 00:00:00+00:00        6.062\n",
       "2       NO1           solar 2022-01-01 01:00:00+00:00        4.697\n",
       "3       NO1           solar 2022-01-01 02:00:00+00:00       10.907\n",
       "4       NO1           solar 2022-01-01 03:00:00+00:00        5.975"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fetch production data for 2022â€“2024\n",
    "YEARS_PROD = [2022, 2023, 2024]\n",
    "\n",
    "total_months = len(PRICE_AREAS) * len(prod_group_ids) * len(YEARS_PROD) * 12\n",
    "print(f\"Planned requests (prod): {total_months} months \"\n",
    "      f\"= {len(PRICE_AREAS)} areas Ã— {len(prod_group_ids)} groups Ã— {len(YEARS_PROD)} years Ã— 12 months\")\n",
    "\n",
    "parts, runs, non_empty, row_count = [], 0, 0, 0\n",
    "pbar = tqdm(total=total_months, desc=\"Production (2022â€“2024) months\", leave=True)\n",
    "\n",
    "for area in PRICE_AREAS:\n",
    "    for g in prod_group_ids:\n",
    "        for y, m in itertools.product(YEARS_PROD, range(1, 13)):\n",
    "            df_m = fetch_month_prod(area, g, y, m)\n",
    "            runs += 1\n",
    "            if not df_m.empty:\n",
    "                parts.append(df_m)\n",
    "                non_empty += 1\n",
    "                row_count += len(df_m)\n",
    "            # update progress every month\n",
    "            pbar.set_postfix_str(f\"rows_so_far={row_count:,}\")\n",
    "            pbar.update(1)\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "prod_2224 = (pd.concat(parts, ignore_index=True)\n",
    "             if parts else pd.DataFrame(columns=[\"priceArea\",\"productionGroup\",\"startTime\",\"quantityKwh\"]))\n",
    "prod_2224 = prod_2224.drop_duplicates(subset=[\"priceArea\",\"productionGroup\",\"startTime\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n=== PRODUCTION 2022â€“2024 ===\")\n",
    "print(f\"Requests run: {runs} | Non-empty months: {non_empty} | Rows: {len(prod_2224):,}\")\n",
    "print(\"Span:\", prod_2224[\"startTime\"].min() if not prod_2224.empty else None,\n",
    "      \"â†’\",  prod_2224[\"startTime\"].max() if not prod_2224.empty else None)\n",
    "display(prod_2224.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e83b4980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Planned requests (cons): 1920 months = 5 areas Ã— 8 groups Ã— 4 years Ã— 12 months\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5d1db6e494e4c93ae775afd0715bfa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Consumption (2021â€“2024) months:   0%|          | 0/1920 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CONSUMPTION 2021â€“2024 ===\n",
      "Requests run: 1920 | Non-empty months: 1200 | Rows: 876,600\n",
      "Span: 2020-12-31 23:00:00+00:00 â†’ 2024-12-31 22:00:00+00:00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>priceArea</th>\n",
       "      <th>consumptionGroup</th>\n",
       "      <th>startTime</th>\n",
       "      <th>quantityKwh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NO1</td>\n",
       "      <td>household</td>\n",
       "      <td>2020-12-31 23:00:00+00:00</td>\n",
       "      <td>2366888.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NO1</td>\n",
       "      <td>household</td>\n",
       "      <td>2021-01-01 00:00:00+00:00</td>\n",
       "      <td>2325218.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NO1</td>\n",
       "      <td>household</td>\n",
       "      <td>2021-01-01 01:00:00+00:00</td>\n",
       "      <td>2273791.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NO1</td>\n",
       "      <td>household</td>\n",
       "      <td>2021-01-01 02:00:00+00:00</td>\n",
       "      <td>2221311.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NO1</td>\n",
       "      <td>household</td>\n",
       "      <td>2021-01-01 03:00:00+00:00</td>\n",
       "      <td>2188174.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  priceArea consumptionGroup                 startTime  quantityKwh\n",
       "0       NO1        household 2020-12-31 23:00:00+00:00    2366888.8\n",
       "1       NO1        household 2021-01-01 00:00:00+00:00    2325218.2\n",
       "2       NO1        household 2021-01-01 01:00:00+00:00    2273791.2\n",
       "3       NO1        household 2021-01-01 02:00:00+00:00    2221311.8\n",
       "4       NO1        household 2021-01-01 03:00:00+00:00    2188174.2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fetch consumption data for 2021â€“2024\n",
    "YEARS_CONS = [2021, 2022, 2023, 2024]\n",
    "\n",
    "total_months_c = len(PRICE_AREAS) * len(cons_group_ids) * len(YEARS_CONS) * 12\n",
    "print(f\"\\nPlanned requests (cons): {total_months_c} months \"\n",
    "      f\"= {len(PRICE_AREAS)} areas Ã— {len(cons_group_ids)} groups Ã— {len(YEARS_CONS)} years Ã— 12 months\")\n",
    "\n",
    "parts_c, runs_c, non_empty_c, row_count_c = [], 0, 0, 0\n",
    "pbar_c = tqdm(total=total_months_c, desc=\"Consumption (2021â€“2024) months\", leave=True)\n",
    "\n",
    "for area in PRICE_AREAS:\n",
    "    for g in cons_group_ids:\n",
    "        for y, m in itertools.product(YEARS_CONS, range(1, 13)):\n",
    "            df_m = fetch_month_cons(area, g, y, m)\n",
    "            runs_c += 1\n",
    "            if not df_m.empty:\n",
    "                parts_c.append(df_m)\n",
    "                non_empty_c += 1\n",
    "                row_count_c += len(df_m)\n",
    "            pbar_c.set_postfix_str(f\"rows_so_far={row_count_c:,}\")\n",
    "            pbar_c.update(1)\n",
    "\n",
    "pbar_c.close()\n",
    "\n",
    "cons_2124 = (pd.concat(parts_c, ignore_index=True)\n",
    "             if parts_c else pd.DataFrame(columns=[\"priceArea\",\"consumptionGroup\",\"startTime\",\"quantityKwh\"]))\n",
    "cons_2124 = cons_2124.drop_duplicates(subset=[\"priceArea\",\"consumptionGroup\",\"startTime\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n=== CONSUMPTION 2021â€“2024 ===\")\n",
    "print(f\"Requests run: {runs_c} | Non-empty months: {non_empty_c} | Rows: {len(cons_2124):,}\")\n",
    "print(\"Span:\", cons_2124[\"startTime\"].min() if not cons_2124.empty else None,\n",
    "      \"â†’\",  cons_2124[\"startTime\"].max() if not cons_2124.empty else None)\n",
    "display(cons_2124.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f55d517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in ind320: ['consumption_mba_hour', 'production_mba_hour']\n"
     ]
    }
   ],
   "source": [
    "# [CASSANDRA] Connect + ensure tables \n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.auth import PlainTextAuthProvider\n",
    "\n",
    "CASS_HOST = \"127.0.0.1\"\n",
    "CASS_PORT = 9042\n",
    "KEYSPACE  = \"ind320\"\n",
    "\n",
    "auth_provider = None  \n",
    "\n",
    "cluster = Cluster([CASS_HOST], port=CASS_PORT, auth_provider=auth_provider)\n",
    "session = cluster.connect()\n",
    "\n",
    "# Create keyspace/tables idempotently\n",
    "session.execute(\"\"\"\n",
    "CREATE KEYSPACE IF NOT EXISTS ind320\n",
    "  WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1}\n",
    "\"\"\")\n",
    "\n",
    "session.set_keyspace(KEYSPACE)\n",
    "\n",
    "session.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS production_mba_hour (\n",
    "  price_area text,\n",
    "  production_group text,\n",
    "  year int,\n",
    "  start_time timestamp,\n",
    "  quantity_kwh double,\n",
    "  PRIMARY KEY ((price_area, production_group, year), start_time)\n",
    ") WITH CLUSTERING ORDER BY (start_time ASC)\n",
    "\"\"\")\n",
    "\n",
    "session.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS consumption_mba_hour (\n",
    "  price_area text,\n",
    "  consumption_group text,\n",
    "  year int,\n",
    "  start_time timestamp,\n",
    "  quantity_kwh double,\n",
    "  PRIMARY KEY ((price_area, consumption_group, year), start_time)\n",
    ") WITH CLUSTERING ORDER BY (start_time ASC)\n",
    "\"\"\")\n",
    "\n",
    "# Quick check\n",
    "rows = session.execute(\"SELECT table_name FROM system_schema.tables WHERE keyspace_name=%s\", [KEYSPACE])\n",
    "print(\"Tables in ind320:\", sorted([r.table_name for r in rows]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865e3a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROD rows: 657600 range: 2021-12-31 23:00:00 â†’ 2024-12-31 22:00:00\n",
      "CONS rows: 876600 range: 2020-12-31 23:00:00 â†’ 2024-12-31 22:00:00\n"
     ]
    }
   ],
   "source": [
    "# [NORMALIZE] prod_2224 and cons_2124 to canonical columns \n",
    "import pandas as pd\n",
    "from pandas.errors import ParserError\n",
    "\n",
    "def to_cassandra_prod(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = pdf.copy()\n",
    "    df[\"price_area\"]       = df[\"priceArea\"]\n",
    "    df[\"production_group\"] = df[\"productionGroup\"]\n",
    "    df[\"start_time\"]       = pd.to_datetime(df[\"startTime\"], utc=True, errors=\"coerce\").dt.tz_localize(None)\n",
    "    df[\"quantity_kwh\"]     = pd.to_numeric(df[\"quantityKwh\"], errors=\"coerce\")\n",
    "    df = df[[\"price_area\",\"production_group\",\"start_time\",\"quantity_kwh\"]].dropna()\n",
    "    df[\"year\"]             = df[\"start_time\"].dt.year.astype(int)\n",
    "    return df\n",
    "\n",
    "def to_cassandra_cons(pdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = pdf.copy()\n",
    "    df[\"price_area\"]        = df[\"priceArea\"]\n",
    "    df[\"consumption_group\"] = df[\"consumptionGroup\"]\n",
    "    df[\"start_time\"]        = pd.to_datetime(df[\"startTime\"], utc=True, errors=\"coerce\").dt.tz_localize(None)\n",
    "    df[\"quantity_kwh\"]      = pd.to_numeric(df[\"quantityKwh\"], errors=\"coerce\")\n",
    "    df = df[[\"price_area\",\"consumption_group\",\"start_time\",\"quantity_kwh\"]].dropna()\n",
    "    df[\"year\"]              = df[\"start_time\"].dt.year.astype(int)\n",
    "    return df\n",
    "\n",
    "prod_cas = to_cassandra_prod(prod_2224)\n",
    "cons_cas = to_cassandra_cons(cons_2124)\n",
    "\n",
    "print(\"PROD rows:\", len(prod_cas), \"range:\", prod_cas[\"start_time\"].min(), \"â†’\", prod_cas[\"start_time\"].max())\n",
    "print(\"CONS rows:\", len(cons_cas), \"range:\", cons_cas[\"start_time\"].min(), \"â†’\", cons_cas[\"start_time\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6fc741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Cassandra upserts completed.\n"
     ]
    }
   ],
   "source": [
    "# QUIET Cassandra bulk upsert (minimal output, safe to rerun)\n",
    "from cassandra.concurrent import execute_concurrent_with_args\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# Silence noisy Python-side Cassandra logs for this cell\n",
    "logging.getLogger(\"cassandra\").setLevel(logging.ERROR)\n",
    "\n",
    "# Ensure keyspace is active \n",
    "try:\n",
    "    session.set_keyspace(\"ind320\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Prepared statements (idempotent to re-prepare)\n",
    "prep_prod = session.prepare(\"\"\"\n",
    "INSERT INTO ind320.production_mba_hour\n",
    "(price_area, production_group, year, start_time, quantity_kwh)\n",
    "VALUES (?, ?, ?, ?, ?)\n",
    "\"\"\")\n",
    "prep_cons = session.prepare(\"\"\"\n",
    "INSERT INTO ind320.consumption_mba_hour\n",
    "(price_area, consumption_group, year, start_time, quantity_kwh)\n",
    "VALUES (?, ?, ?, ?, ?)\n",
    "\"\"\")\n",
    "\n",
    "def _to_native_args(df, cols):\n",
    "    out = []\n",
    "    for pa, grp, yr, ts, qty in df[cols].itertuples(index=False, name=None):\n",
    "        ts = pd.to_datetime(ts, utc=True).tz_localize(None).to_pydatetime()  # naive UTC\n",
    "        out.append((str(pa), str(grp), int(yr), ts, float(qty)))\n",
    "    return out\n",
    "\n",
    "prod_args = _to_native_args(\n",
    "    prod_cas, [\"price_area\",\"production_group\",\"year\",\"start_time\",\"quantity_kwh\"]\n",
    ")\n",
    "cons_args = _to_native_args(\n",
    "    cons_cas, [\"price_area\",\"consumption_group\",\"year\",\"start_time\",\"quantity_kwh\"]\n",
    ")\n",
    "\n",
    "# Execute quietly (no per-row logging, no COUNT(*))\n",
    "_ = execute_concurrent_with_args(session, prep_prod, prod_args, concurrency=256, raise_on_first_error=False)\n",
    "_ = execute_concurrent_with_args(session, prep_cons, cons_args, concurrency=256, raise_on_first_error=False)\n",
    "\n",
    "print(\"Cassandra upserts completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7fbc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mongo PRODUCTION upserts: 657575  |  updates: 0  |  total docs now: 657575\n",
      "Mongo CONSUMPTION upserts: 876600  |  updates: 0  |  total docs now: 876600\n"
     ]
    }
   ],
   "source": [
    "# [MONGODB] bulk upsert to two collections \n",
    "from pymongo import MongoClient, UpdateOne\n",
    "import os\n",
    "import streamlit as st\n",
    "from pymongo import MongoClient\n",
    "\n",
    "MONGO_URI = st.secrets[\"MONGO_URI\"]\n",
    "MDB_NAME  = st.secrets[\"MONGO_DB\"]\n",
    "\n",
    "COL_PROD  = \"elhub_production_mba_hour\"\n",
    "COL_CONS  = \"elhub_consumption_mba_hour\"\n",
    "    \n",
    "client = MongoClient(MONGO_URI)\n",
    "mdb = client[MDB_NAME]\n",
    "\n",
    "prod_col = mdb[COL_PROD]\n",
    "cons_col = mdb[COL_CONS]\n",
    "\n",
    "# indexes (unique compound ensures idempotence)\n",
    "prod_col.create_index(\n",
    "    [(\"price_area\",1), (\"production_group\",1), (\"start_time\",1)],\n",
    "    unique=True, name=\"uniq_prod_area_group_time\"\n",
    ")\n",
    "cons_col.create_index(\n",
    "    [(\"price_area\",1), (\"consumption_group\",1), (\"start_time\",1)],\n",
    "    unique=True, name=\"uniq_cons_area_group_time\"\n",
    ")\n",
    "\n",
    "def upsert_df(df, col, key_fields):\n",
    "    ops = []\n",
    "    for rec in df.to_dict(orient=\"records\"):\n",
    "        key = {k: rec[k] for k in key_fields}\n",
    "        ops.append(UpdateOne(key, {\"$set\": rec}, upsert=True))\n",
    "    if not ops:\n",
    "        return (0,0)\n",
    "    res = col.bulk_write(ops, ordered=False)\n",
    "    return (res.upserted_count or 0, res.modified_count or 0)\n",
    "\n",
    "\n",
    "# PRODUCTION: only years 2022â€“2024 (append to our existing 2021 in Mongo)\n",
    "prod_mongo = prod_cas[prod_cas[\"year\"].between(2022, 2024)]\n",
    "u, m = upsert_df(prod_mongo, prod_col, [\"price_area\",\"production_group\",\"start_time\"])\n",
    "print(f\"Mongo PRODUCTION upserts: {u}  |  updates: {m}  |  total docs now: {prod_col.estimated_document_count()}\")\n",
    "\n",
    "\n",
    "# CONSUMPTION: 2021â€“2024 full load\n",
    "u, m = upsert_df(cons_cas, cons_col, [\"price_area\",\"consumption_group\",\"start_time\"])\n",
    "print(f\"Mongo CONSUMPTION upserts: {u}  |  updates: {m}  |  total docs now: {cons_col.estimated_document_count()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IND320env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
